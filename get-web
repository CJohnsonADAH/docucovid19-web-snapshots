#!/bin/bash
#
# get-web: script to orchestrate the download of contents, metadata, and
# screenshot of a website that you want to archive. This is typically
# invoked by the `get-all` script, which handles questions of scheduling
# and shared parameters.
#
# @version 2020.0429

SCRIPT_PATH=$( readlink -f "$0" )
SCRIPT_NAME=$( basename "${SCRIPT_PATH}" )
SCRIPT_DIR=$( dirname "${SCRIPT_PATH}" )
SCREENCAP_DIR="${SCRIPT_DIR}/screencaps"
PATH="${SCRIPT_DIR}:${SCREENCAP_DIR}:${PATH}"
SCREENCAP_PID=""

function do_usage_error () {
	echo "Usage: ${SCRIPT_NAME} URL SITE_TAG DATETIME DATA_DIR STEP STEPCYCLE DURATION [--WGET-PARAMETER=VALUE...]" 1>&2
}

## Process command-line arguments. This is a bit complicated, because
## many arguments are passed along as-is to wget.

declare -a GW_ARGV=("$0")
declare -a GW_OBJS=()
declare -A GW_SWITCHES=()
declare -A GW_PARAMS=()

while [ $# -gt 0 ] ; do
	GW_ARGV=("${GW_ARGV[@]}" "$1")
	if [[ "$1" =~ ^--([^=]+)(=(.*))?$ ]] ; then
		KEY="${BASH_REMATCH[1]}"
		GW_SWITCHES["${KEY}"]="${BASH_REMATCH[0]}"
		GW_PARAMS["${KEY}"]="${KEY}"
		if [ ! -z "${BASH_REMATCH[2]}" ] ; then
			GW_PARAMS["${KEY}"]="${BASH_REMATCH[3]}"
		fi
	else
		GW_OBJS=("${GW_OBJS[@]}" "$1")
	fi
	shift
done

if [ "${GW_PARAMS[test]}" = "argv" ] ; then
	declare -p GW_ARGV
	declare -p GW_OBJS
	declare -p GW_SWITCHES
	declare -p GW_PARAMS
	exit
fi

DATA_URL="${GW_OBJS[0]}"
SITE_TAG="${GW_OBJS[1]}"
DATESTAMP="${GW_OBJS[2]}"
DATA_DIR="${GW_OBJS[3]}"
STEP="${GW_OBJS[4]}"
STEPMOD="${GW_OBJS[5]}"
DURATION="${GW_OBJS[6]}"

if [[ ! -z "${GW_PARAMS[first]}" ]] ; then
	DATESTAMP=""
	STEP=0
	STEPMOD=4
fi

declare -A WG_ARGV=()

WG_INCLUDE=""
WG_EXCLUDE="--no-parent"

WG_ARGV[no-verbose]="--no-verbose"
WG_ARGV[no-check-certificate]="" # off by default; individual sites can turn it on
WG_ARGV[recursive]="--recursive"
WG_ARGV[level]="--level=1"
WG_ARGV[timestamping]="--timestamping"
WG_ARGV[convert-links]="--convert-links"
WG_ARGV[page-requisites]="--page-requisites"
WG_ARGV[backup-converted]="--backup-converted"
WG_ARGV[adjust-extension]="--adjust-extension"
WG_ARGV[restrict-file-names]="--restrict-file-names=windows"
WG_ARGV[timeout]="--timeout=20"
WG_ARGV[tries]="--tries=10"
WG_ARGV[reject_xmlrpc]="--reject=xmlrpc.php"

for KEY in "${!GW_SWITCHES[@]}" ; do
	SWITCH="${GW_SWITCHES[${KEY}]}"
	if [[ "${SWITCH}" =~ ^--include-directories=(.*) ]] ; then
		WG_INCLUDE="${SWITCH}"
		if [ "${BASH_REMATCH[1]}" = "-" ] ; then
			WG_INCLUDE=""
			WG_EXCLUDE=""
		elif [ "${BASH_REMATCH[1]}" != "*" ] ; then
			WG_EXCLUDE="--exclude-directories=*"
		else
			WG_INCLUDE=""
			WG_EXCLUDE=""
		fi
	elif [[ "${KEY}" =~ ^(first)$ ]] ; then
		noop # this is an argument to get-web, not wget
	else
		WG_ARGV["${KEY}"]="${SWITCH}"
	fi
done

if [ -z "${STEP}" ] ; then
	STEP=0
fi
if [ -z "${STEPMOD}" ] ; then
	STEPMOD=1
fi

POLL_INTERVAL=$( get-all-polling-interval --hours )
DAYSTEPS=$(( 24 / POLL_INTERVAL ))
SITESTEP=$(( STEP % STEPMOD ))
RATIO=$(( STEPMOD / DAYSTEPS ))
if [ "${RATIO}" -eq 0 ] ; then
	RATIO=1
fi
OFFCYCLE=$(( SITESTEP % RATIO ))

if [[ "${GW_PARAMS[test]}" =~ (^|,)params(,|$) ]] ; then
	printf "DATA_URL: %s\n" "${DATA_URL}"
	printf "SITE_TAG: %s\n" "${SITE_TAG}"
	printf "DATESTAMP: %s\n" "${DATESTAMP}"
	printf "DATA_DIR: %s\n" "${DATA_DIR}"
	printf "STEP: %d\n" "${STEP}"
	printf "STEPMOD: %d\n" "${STEPMOD}"
	printf "DURATION: %d\n" "${DURATION}"
	printf "DAYSTEPS: %d\n" "${DAYSTEPS}"
	printf "SITESTEP: %d\n" "${SITESTEP}"
	printf "RATIO: %d\n" "${RATIO}"
	printf "OFFCYCLE: %d\n" "${OFFCYCLE}"
	printf "\n"
	printf "WG_INCLUDE=%s\n" "${WG_INCLUDE}"
	printf "WG_EXCLUDE=%s\n" "${WG_EXCLUDE}"
	declare -p WG_ARGV
	exit
fi

if [[ -z "${DATA_URL}" ]] ; then
	echo "[${SCRIPT_NAME}] No URL provided." 1>&2
	do_usage_error
	exit 255
fi
if [[ -z "${SITE_TAG}" ]] ; then
	echo "[${SCRIPT_NAME}] No site name provided." 1>&2
	do_usage_error
	exit 255
fi
if [ -z "${DATA_DIR}" ] ; then
	DATA_DIR="${HOME}/covid-data"
fi
if [ -z "${DATESTAMP}" ] ; then
	DATESTAMP=$( TZ="UTC" date +"%Y%m%d%H%M%SZ" )
fi


if [[ "${OFFCYCLE}" -gt 0 ]] ; then
	TS_TODAY=$( date )
	printf "\n[...] Deferring HTML front page dump of %s (%d-day cycle): %s [...]\n" "${DATA_URL}" "${RATIO}" "${TS_TODAY}"
	exit
fi

HTMLPAGE_DATA="${DATA_DIR}/html"
if [ ! -d "${HTMLPAGE_DATA}/${DATESTAMP}" ] ; then
	mkdir --parents "${HTMLPAGE_DATA}/${DATESTAMP}"
fi

if [ ! -z "${SITE_TAG}" ] ; then
	HTMLPAGE_DATA="${HTMLPAGE_DATA}/${DATESTAMP}/${SITE_TAG}"
fi
HTMLPAGE_DATA="${HTMLPAGE_DATA}-"

export DATA_DIR
export SITE_TAG
export DATESTAMP
export SITESTEP
export WG_INCLUDE
export WG_EXCLUDE
export WG_ARGV
export DATA_URL
export DATA_URLS
export DATA_OUT
export WARC_OUT
export WARC_SITE_OUT

export DATA_PNG
export DATA_PNG_N
export DURATION
export SCREENCAP_DIR
export SCREENCAP_PID

function do_html_snapshot () {
	wget ${WG_ARGV[no-verbose]} ${WG_ARGV[no-check-certificate]} --output-document="${DATA_OUT}" --warc-file="${WARC_OUT}" "${DATA_URL}"

	if [ -r "${DATA_OUT}" ] ; then get-checksum "${DATA_OUT}" ; fi
	if [ -r "${WARC_OUT}.warc" ] ; then get-checksum "${WARC_OUT}.warc" ; fi
}

function do_website_screenshot () {
	LWD="${PWD}"
	cd "${SCREENCAP_DIR}"
	IDX=0
	for URL in "${DATA_URLS[@]}" ; do
		OUT_PNG="${DATA_PNG}"
		if [ "${IDX}" -gt 0 ] ; then
			wait "${SCREENCAP_PID}"
			echo "... Screencap completed."

			OUT_PNG=$( printf "${DATA_PNG_N}" "${IDX}" )
		fi
		node --unhandled-rejections=strict ./screencap.js --headless "${URL}" "${OUT_PNG}" "${DURATION}" & # this can take a while, do it in the background
		SCREENCAP_PID="$!"

		IDX=$((IDX + 1))
	done
	cd "${LWD}"
}

function do_webarc_mirror () {
	WG_WARC=""
	if [[ "${SITESTEP}" -eq 0 ]] ; then
		WG_WARC="--warc-file=${WARC_SITE_OUT}"
	fi

	cd "${DATA_DIR}"

	MIRROR_DIR="${DATA_DIR}/mirror_${SITE_TAG}"
	if [ ! -d "${MIRROR_DIR}" ] ; then
		mkdir --verbose "${MIRROR_DIR}"
	fi
	cd "${MIRROR_DIR}"

	if [ ! -d "./warc" ] ; then
		mkdir ./warc
	fi
	mv *.warc *.warc.* ./warc

	echo '$' wget  ${WG_INCLUDE} ${WG_EXCLUDE} ${WG_WARC} ${WG_ARGV[@]} "${DATA_URLS[@]}"

	wget ${WG_INCLUDE} ${WG_EXCLUDE} ${WG_WARC} ${WG_ARGV[@]} "${DATA_URLS[@]}"

	if [ ! -z "${WG_WARC}" ] ; then
		if [ -r "${WARC_SITE_OUT}.warc" ] ; then
			get-checksum "${WARC_SITE_OUT}.warc"
		fi
	fi

	SNAPSHOTDIR=${DATA_DIR}/html/snapshots/${SITE_TAG}
	if [ ! -d "${SNAPSHOTDIR}" ] ; then
		mkdir --parents "${SNAPSHOTDIR}"
	fi

	cd "${MIRROR_DIR}"
	find -name '*.warc' -exec gzip \{\} \;

	echo '$' make-snapshot.pl "${MIRROR_DIR}/" "${SNAPSHOTDIR}" --timestamp="${DATESTAMP}"
	make-snapshot.pl "${MIRROR_DIR}/" "${SNAPSHOTDIR}" --timestamp="${DATESTAMP}" --exclude=warc
} # /function do_webarc_mirror

# fetch the basic HTML

URL_OUT="${HTMLPAGE_DATA}${DATESTAMP}.url.txt"
DATA_OUT="${HTMLPAGE_DATA}${DATESTAMP}.html"
DATA_PNG="${HTMLPAGE_DATA}${DATESTAMP}.png"
DATA_PNG_N="${HTMLPAGE_DATA}${DATESTAMP}-%d.png"
HEAD_OUT="${HTMLPAGE_DATA}${DATESTAMP}.head"
WARC_OUT="${HTMLPAGE_DATA}${DATESTAMP}"
WARC_SITE_OUT="${SITE_TAG}-${DATESTAMP}"

IFS="|" read -r -a DATA_URLS <<< "${DATA_URL}"
DATA_URL="${DATA_URLS[0]}"
echo "${DATA_URL}" > "${URL_OUT}" 

if [[ ! -z "${WG_ARGV[domains]}" ]] ; then
	WG_ARGV[domains]=$( get-wget-domains-switch "${WG_ARGV[domains]}" "${DATA_URL}" )
fi

do_html_snapshot
do_website_screenshot
do_webarc_mirror &

if [[ ! -z "${SCREENCAP_PID}" && -r "/proc/${SCREENCAP_PID}" ]] ; then
	wait "${SCREENCAP_PID}"
	echo "... Screencap completed."
fi

echo "Completed HTML front page dump of ${DATA_URL}: " $( date )

