#!/bin/bash

export SCRIPT_PATH=$( readlink -f "$0" )
export SCRIPT_DIR=$( dirname "${SCRIPT_PATH}" )
export PATH="${SCRIPT_DIR}:${PATH}"

export DATA_SERVER="alacovdat.dev.radgeek.net"
export DATA_SLUG="covid-data"
export DATA_DIR="${HOME}/${DATA_SLUG}"

if [[ ! -d "${DATA_DIR}" ]] ; then
	mkdir --parents --verbose "${DATA_DIR}"
fi
cd "${DATA_DIR}"

SYNC=0
DATAONLY=0
WEBONLY=0
if [ ! -z "$1" ] ; then
	if [ "$1" == "--sync" ] ; then
		SYNC=1
		shift
	fi

	if [ "$1" == "--data" ] ; then
		DATAONLY=1
		shift
	elif [ "$1" == "--web" ] ; then
		WEBONLY=1
		shift
	fi
fi

FILTER_SLUG=""
if [ ! -z "$1" ] ; then
	FILTER_SLUG="$1"
fi

if [ "${SYNC}" -ne 0 ] ; then
	echo "=== Sync: Preparing for Transmit ==="
	find -name '*.warc' -exec gzip \{\} \;
	echo "... transmitting via rsync ..."
	if [ "${DATAONLY}" -eq 0 ] ; then
		if [ "${WEBONLY}" -eq 0 ] ; then
			rsync --archive --itemize-changes --progress --exclude="mirror_*" --hard-links --exclude="${DATA_SERVER}" ./ ${DATA_SERVER}:${DATA_SLUG}/
		else
			rsync --archive --itemize-changes --progress --hard-links ./html/ ${DATA_SERVER}:${DATA_SLUG}/html/
		fi
	else
		rsync --archive --itemize-changes --progress --exclude="mirror_*" --hard-links ./data/ ${DATA_SERVER}:${DATA_SLUG}/data/

	fi
	exit
fi

if [[ "${DATAONLY}" -eq 0 ]] ; then
	if [[ -z "${FILTERSLUG}" ]] ; then
		STEP_FILE="${DATA_DIR}/snapshot-step.txt"
		STEP_PERIOD=4

		if [ -r "${STEP_FILE}" ] ; then
			STEP=$( cat "${DATA_DIR}/snapshot-step.txt" )
		else
			STEP=3
		fi
		STEP=$(( (STEP + 1) % STEP_PERIOD ))
		printf "%d\n" "${STEP}" > "${STEP_FILE}"

		printf "*** Step %d of %d ...\n" "${STEP}" "${STEP_PERIOD}"
	fi
fi

DATESTAMP=$( TZ="UTC" date +"%Y%m%d%H%M%SZ" )

if [[ "${WEBONLY}" -eq 0 ]] ; then

	printf "*** Getting data ...\n"
	DATA_SOURCES="data-sources.tsv.txt"
	DATA_SOURCES_URL="${DATA_SERVER}/${DATA_SOURCES}"

wget --mirror "http://${DATA_SOURCES_URL}"
while IFS="" read LINE; do
	DATA_SLUG=$( printf "%s" "$LINE" | cut -f 1 )
	DATA_PATH=$( printf "%s" "$LINE" | cut -f 2 )
	DATA_PARAM=$( printf "%s" "$LINE" | cut -f 3 )

	if [ ! -z "${DATA_PATH}" ] ; then
		if [ -z "${FILTER_SLUG}" -o "${DATA_SLUG}" = "${FILTER_SLUG}" ] ; then

		DATA_URL="${DATA_PATH}"
		if [ ! -z "${DATA_PARAM}" ] ; then
			DATA_URL="${DATA_URL}?f=json&${DATA_PARAM}"
		fi
		
		echo '$' get-data "${DATA_URL}" "${DATA_SLUG}" "${DATESTAMP}" "${DATA_DIR}"

		get-data "${DATA_URL}" "${DATA_SLUG}" "${DATESTAMP}" "${DATA_DIR}"

		fi

	fi
done < "${DATA_DIR}/${DATA_SOURCES_URL}"


fi

if [[ "${DATAONLY}" -eq 0 ]] ; then

wget --mirror "http://${DATA_SERVER}/sources.tsv.txt"
while IFS="" read LINE; do
	WWW_PERIOD=$( printf "%s" "$LINE" | cut -f 1 )
	WWW_SLUG=$( printf "%s" "$LINE" | cut -f 2 )
	WWW_URL=$( printf "%s" "$LINE" | cut -f 3 )

	declare -a WWW_SWITCHES=()
	FIELD=4
	WWW_SWITCH="dummy"
	while [ ! -z "${WWW_SWITCH}" ] ; do
		WWW_SWITCH=$( printf "%s" "$LINE" | cut -f "${FIELD}" )
		if [ ! -z "${WWW_SWITCH}" ] ; then
			WWW_SWITCHES=(${WWW_SWITCHES[@]} "${WWW_SWITCH}")
		fi
		FIELD=$((FIELD+1))
	done

	if [ ! -z "${WWW_URL}" ] ; then
		if [ -z "${FILTER_SLUG}" -o "${WWW_SLUG}" = "${FILTER_SLUG}" ] ; then

		get-web "${WWW_URL}" "${WWW_SLUG}" "${DATESTAMP}" "${DATA_DIR}" "${STEP}" "${WWW_PERIOD}" ${WWW_SWITCHES[@]}

		fi
	fi

	unset WWW_SWITCHES
done < "${DATA_DIR}/${DATA_SERVER}/sources.tsv.txt"

echo "Completed web snapshots: " $( date )

fi

cd "${DATA_DIR}"
find -name '*.warc' -not -path '*/html/snapshots/*' -exec gzip \{\} \;

exit

